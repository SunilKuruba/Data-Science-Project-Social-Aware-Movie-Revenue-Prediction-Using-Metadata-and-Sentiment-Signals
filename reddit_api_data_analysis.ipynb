{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Movie Sentiment Analysis - Data Collection / Sentiment + Emotion Analysis Script\n",
    "\n",
    "## By Leonardo Ferreira\n",
    "\n",
    "This script collects data from reddit related to a given movie and performs sentiment and emotion analysis using a widely used [pretrained model for sentiment analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and a [pretrained emotion recognition model](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion) to generate the scores.\n",
    "\n",
    "### Requirements\n",
    "- `praw` (python reddit API wrapper)\n",
    "- `pandas`\n",
    "- `nltk`\n",
    "- `datetime`\n",
    "- `torch`\n",
    "- `scipy`\n",
    "- `transformers`\n",
    "- `python-dotenv` - if you want to store your API credentials in a .env file\n",
    "\n",
    "# ⚠️ IMPORTANT: In order to use this script you must have access to reddit API\n",
    "\n",
    "## Create a reddit account\n",
    "\n",
    "- If you don't already have one, go to reddit's registration page: [https://www.reddit.com/register/](https://www.reddit.com/register/)\n",
    "\n",
    "## Create a reddit application\n",
    "\n",
    "- Go to your [App Preferences page](https://www.reddit.com/prefs/apps) while logged in.\n",
    "- Scroll down to the bottom and click **\"create another app\"** (or **\"create app\"** if it's your first one).\n",
    "\n",
    "## Fill in the application details\n",
    "\n",
    "- Select **\"script\"** as the application type.\n",
    "- Provide a name for your application (e.g., \"Movie Sentiment Analysis Project\").\n",
    "- Add a brief description.\n",
    "- For the **\"about url\"** and **\"redirect uri\"** fields, you can use `http://localhost:8080` as a placeholder.\n",
    "- Click **\"create app\"** to submit.\n",
    "\n",
    "## Get your credentials\n",
    "\n",
    "- After creating the app, you'll see the **client ID** directly under the app name.\n",
    "- The **client secret** will be displayed as **\"secret\"**.\n",
    "- Make note of both, as you'll need them in your code.\n",
    "\n",
    "## Example: Initializing the reddit API with PRAW\n",
    "\n",
    "```python\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "    user_agent=\"python:movie.sentiment.analyzer:v1.0 (by /u/your_username)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "# this is used to import private variables from reddit -> you can either modify the script using your own credentials or create a .env with them.\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditMovieDataCollector:\n",
    "    def __init__(self, client_id, client_secret, user_agent):\n",
    "        \"\"\"\n",
    "        initialize the reddit API client\n",
    "        \n",
    "        parameters:\n",
    "        - client_id: your reddit API client ID\n",
    "        - client_secret: your reddit API client secret\n",
    "        - user_agent: unique identifier for your script\n",
    "        \"\"\"\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "\n",
    "\n",
    "        # initialize pretrained sentiment analysis model\n",
    "        # using RoBERTa model fine-tuned for sentiment analysis on tweets\n",
    "        # source: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "        self.sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model_name)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model_name)\n",
    "\n",
    "\n",
    "        # doing the same but for emotion analyzis\n",
    "        # source: https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion\n",
    "        self.emotion_model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "        self.emotion_tokenizer = AutoTokenizer.from_pretrained(self.emotion_model_name)\n",
    "        self.emotion_model = AutoModelForSequenceClassification.from_pretrained(self.emotion_model_name)\n",
    "\n",
    "        # get emotion labels dynamically from the model configuration\n",
    "        self.emotion_labels = [\n",
    "            self.emotion_model.config.id2label[i] \n",
    "            for i in range(len(self.emotion_model.config.id2label))\n",
    "        ]\n",
    "\n",
    "        # create emotion column names\n",
    "        self.emotion_columns = [f\"{label.lower()}_emotion\" for label in self.emotion_labels]\n",
    "\n",
    "\n",
    "        # create folder for data if it doesn't exist\n",
    "        if not os.path.exists('movie_data'):\n",
    "            os.makedirs('movie_data')\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        clean and preprocess text data\n",
    "        \n",
    "        parameters:\n",
    "        - text: text to clean\n",
    "        \n",
    "        returns:\n",
    "        - cleaned text\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove urls\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # keep only letters and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_sentiment_scores(self, text):\n",
    "        \"\"\"\n",
    "        calculate sentiment scores for a given text using VADER\n",
    "        \n",
    "        parameters:\n",
    "        - text: text to analyze\n",
    "        \n",
    "        returns:\n",
    "        - dictionary with sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # return neutral sentiment if text is empty\n",
    "            return {\n",
    "                'compound': 0,\n",
    "                'pos': 0,\n",
    "                'neu': 1,\n",
    "                'neg': 0\n",
    "            }\n",
    "        \n",
    "        # truncate text if it's too long for the model\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        encoded_input = self.sentiment_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # get model output\n",
    "        with torch.no_grad():\n",
    "            output = self.sentiment_model(**encoded_input)\n",
    "        \n",
    "        # get probabilities\n",
    "        scores = softmax(output.logits[0].numpy())\n",
    "        \n",
    "        # map scores to sentiment categories (negative, neutral, positive)\n",
    "        sentiment_scores = {\n",
    "            'neg': float(scores[0]),\n",
    "            'neu': float(scores[1]),\n",
    "            'pos': float(scores[2]),\n",
    "            'compound': float(scores[2] - scores[0])\n",
    "        }\n",
    "        \n",
    "        return sentiment_scores\n",
    "    \n",
    "    def get_emotion_scores(self, text):\n",
    "        \"\"\"\n",
    "        calculate emotion scores\n",
    "\n",
    "        parameters:\n",
    "        - text: text to analyze\n",
    "        \n",
    "        returns:\n",
    "        - dictionary with emotion scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # return neutral emotion scores if text is empty\n",
    "            return {label.lower(): 0 for label in self.emotion_labels}\n",
    "        \n",
    "        # truncate text if it's too long for the model\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        encoded_input = self.emotion_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # get model output\n",
    "        with torch.no_grad():\n",
    "            output = self.emotion_model(**encoded_input)\n",
    "        \n",
    "        # get probabilities\n",
    "        scores = softmax(output.logits[0].numpy())\n",
    "        \n",
    "        # map scores to emotion categories dynamically\n",
    "        emotion_mapping = {\n",
    "            self.emotion_model.config.id2label[i].lower(): float(scores[i]) \n",
    "            for i in range(len(self.emotion_labels))\n",
    "        }\n",
    "        \n",
    "        return emotion_mapping\n",
    "    \n",
    "    def collect_reddit_data(self, movie_name, limit=100, time_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        collect reddit data for a specific movie\n",
    "        \n",
    "        parameters:\n",
    "        - movie_name: name of the movie to search for\n",
    "        - limit: maximum number of posts to retrieve\n",
    "        - time_filter: time period to search within (hour, day, week, month, year, all)\n",
    "        \n",
    "        returns:\n",
    "        - df with collected data\n",
    "        \"\"\"\n",
    "        \n",
    "        posts_data = []\n",
    "        comments_data = []\n",
    "        \n",
    "        # search for posts related to the movie\n",
    "        search_query = movie_name\n",
    "        posts = self.reddit.subreddit(\"all\").search(\n",
    "            search_query, \n",
    "            sort=\"relevance\", \n",
    "            time_filter=time_filter, \n",
    "            limit=limit,\n",
    "            syntax='lucene'\n",
    "        )\n",
    "        \n",
    "        post_count = 0\n",
    "        comment_count = 0\n",
    "        \n",
    "        for post in posts:\n",
    "            post_count += 1\n",
    "            \n",
    "            # clean title and text\n",
    "            clean_title = self.clean_text(post.title)\n",
    "            clean_text = self.clean_text(post.selftext)\n",
    "            combined_text = f\"{clean_title} {clean_text}\"\n",
    "            \n",
    "            # get sentiment scores\n",
    "            sentiment_scores = self.get_sentiment_scores(combined_text)\n",
    "\n",
    "            # get emotion scores\n",
    "            emotion_scores = self.get_emotion_scores(combined_text)\n",
    "\n",
    "            # process post\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': str(post.author),\n",
    "                'score': post.score,\n",
    "                'created_utc': datetime.datetime.fromtimestamp(post.created_utc),\n",
    "                'subreddit': post.subreddit.display_name,\n",
    "                'num_comments': post.num_comments,\n",
    "                'compound_sentiment': sentiment_scores['compound'],\n",
    "                'positive_sentiment': sentiment_scores['pos'],\n",
    "                'neutral_sentiment': sentiment_scores['neu'],\n",
    "                'negative_sentiment': sentiment_scores['neg'],\n",
    "                **{f\"{k}_emotion\": v for k, v in emotion_scores.items()},\n",
    "                'content_type': 'post'\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "            \n",
    "            # get comments while skipping loading more comments to avoid API rate limits\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for comment in post.comments.list():\n",
    "                comment_count += 1\n",
    "                \n",
    "                # clean comment text\n",
    "                clean_comment = self.clean_text(comment.body)\n",
    "                \n",
    "                # get sentiment scores\n",
    "                comment_sentiment = self.get_sentiment_scores(clean_comment)\n",
    "\n",
    "                # get emotion scores\n",
    "                comment_emotion = self.get_emotion_scores(clean_comment)\n",
    "\n",
    "                # process comment\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'post_id': post.id,\n",
    "                    'text': comment.body,\n",
    "                    'author': str(comment.author),\n",
    "                    'score': comment.score,\n",
    "                    'created_utc': datetime.datetime.fromtimestamp(comment.created_utc),\n",
    "                    'subreddit': post.subreddit.display_name,\n",
    "                    'compound_sentiment': comment_sentiment['compound'],\n",
    "                    'positive_sentiment': comment_sentiment['pos'],\n",
    "                    'neutral_sentiment': comment_sentiment['neu'],\n",
    "                    'negative_sentiment': comment_sentiment['neg'],\n",
    "                    **{f\"{k}_emotion\": v for k, v in comment_emotion.items()},\n",
    "                    'content_type': 'comment'\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "            \n",
    "            # sleep to avoid rate limits\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # create dfs\n",
    "        posts_df = pd.DataFrame(posts_data)\n",
    "        comments_df = pd.DataFrame(comments_data)\n",
    "        \n",
    "        # save to csv\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        movie_name_cleaned = movie_name.replace(\" \", \"_\").lower()\n",
    "        \n",
    "        posts_filename = f\"movie_data/{movie_name_cleaned}_posts_{timestamp}.csv\"\n",
    "        comments_filename = f\"movie_data/{movie_name_cleaned}_comments_{timestamp}.csv\"\n",
    "        \n",
    "        posts_df.to_csv(posts_filename, index=False)\n",
    "        comments_df.to_csv(comments_filename, index=False)\n",
    "        \n",
    "        print(f\"Data collection is complete: {post_count} posts and {comment_count} comments\")\n",
    "        print(f\"Data saved to {posts_filename} and {comments_filename}\")\n",
    "        \n",
    "        # Combine data for analysis\n",
    "        all_data = pd.concat([posts_df, comments_df])\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def analyze_sentiment_and_emotion_distribution(self, data):\n",
    "        \"\"\"\n",
    "        analyze the sentiment distribution of collected data\n",
    "        \n",
    "        parameters:\n",
    "        - data: df with collected data\n",
    "        \n",
    "        returns:\n",
    "        - df with sentiment distribution analysis\n",
    "        \"\"\"\n",
    "        # calculate averages for sentiment scores\n",
    "        sentiment_avg = {\n",
    "            'Average compound score': data['compound_sentiment'].mean(),\n",
    "            'Average positive score': data['positive_sentiment'].mean(),\n",
    "            'Average neutral score': data['neutral_sentiment'].mean(),\n",
    "            'Average negative score': data['negative_sentiment'].mean()\n",
    "        }\n",
    "        \n",
    "        # categorize sentiments\n",
    "        data['sentiment_category'] = pd.cut(\n",
    "            data['compound_sentiment'],\n",
    "            bins=[-2, -0.6, -0.2, 0.2, 0.6, 2],\n",
    "            labels=['Very negative', 'Negative', 'Neutral', 'Positive', 'Very positive']\n",
    "        )\n",
    "        \n",
    "        # count by category\n",
    "        sentiment_counts = data['sentiment_category'].value_counts().to_dict()\n",
    "        \n",
    "        # calculate percentages\n",
    "        total = sum(sentiment_counts.values())\n",
    "        sentiment_percentages = {k: (v / total) * 100 for k, v in sentiment_counts.items()}\n",
    "        \n",
    "        # emotion analysis\n",
    "        emotion_columns = [col for col in data.columns if col.endswith('_emotion')]\n",
    "\n",
    "        # calculate average emotion scores\n",
    "        emotion_avg = {f'Average {col.split(\"_\")[0]} emotion': data[col].mean() for col in emotion_columns}\n",
    "        \n",
    "        # calculate total emotion scores across all content\n",
    "        total_emotion_scores = {}\n",
    "        for col in emotion_columns:\n",
    "            emotion_name = col.split('_')[0]\n",
    "            total_emotion_scores[emotion_name] = data[col].sum()\n",
    "        \n",
    "        # sort emotions\n",
    "        sorted_emotions = sorted(total_emotion_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        emotions_dict = {\n",
    "            'emotions': [emotion for emotion, score in sorted_emotions],\n",
    "            'emotions_scores': {emotion: score for emotion, score in sorted_emotions}\n",
    "        }\n",
    "\n",
    "        # calculate percentages for top 5 emotions\n",
    "        total_emotion_score = sum(total_emotion_scores.values())\n",
    "        emotions_percentages = {\n",
    "            emotion: (score / total_emotion_score) * 100 \n",
    "            for emotion, score in emotions_dict['emotions_scores'].items()\n",
    "        }\n",
    "        emotions_dict['emotions_percentages'] = emotions_percentages\n",
    "\n",
    "        # combine results\n",
    "        analysis_result = {\n",
    "            'sentiment_avg': sentiment_avg,\n",
    "            'sentiment_counts': sentiment_counts,\n",
    "            'sentiment_percentages': sentiment_percentages,\n",
    "            'emotion_avg': emotion_avg,\n",
    "            'emotions': emotions_dict\n",
    "        }\n",
    "        \n",
    "        return analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection is complete: 50 posts and 16303 comments\n",
      "Data saved to movie_data/interstellar_posts_20250411_200432.csv and movie_data/interstellar_comments_20250411_200432.csv\n"
     ]
    }
   ],
   "source": [
    "# load credentials from .env file\n",
    "load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "# initialize collector with reddit credentials\n",
    "collector = RedditMovieDataCollector(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT)\n",
    "\n",
    "# collect data for a given movie\n",
    "# change the movie name to get sentiments about different movies\n",
    "movie_name = \"Flash\"\n",
    "# limit = 50 means: 50 posts. Be aware that a post can have multiple comments (API rate limit)\n",
    "limit = 100\n",
    "data = collector.collect_reddit_data(movie_name, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis results:\n",
      "Average sentiment scores:\n",
      "  Average compound score: -0.0386\n",
      "  Average positive score: 0.2636\n",
      "  Average neutral score: 0.4342\n",
      "  Average negative score: 0.3022\n",
      "\n",
      "\n",
      "Sentiment distribution:\n",
      "  Neutral: 5259 (32.16%)\n",
      "  Very negative: 3374 (20.63%)\n",
      "  Negative: 3032 (18.54%)\n",
      "  Very positive: 3005 (18.38%)\n",
      "  Positive: 1683 (10.29%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metrics meaning\n",
    "\n",
    "Sentiment scores:\n",
    "Average compound score: overall sentiment score, ranging from -1 (very negative) to +1 (very positive)\n",
    "Average positive score: represents the proportion of text that expresses positive sentiment\n",
    "Average neutral score: % of the content that is presenting emotionally neutral language\n",
    "Average negative score: % of the content that is presenting emotionally negative language\n",
    "\n",
    "Sentiment distribution:\n",
    "Neutral: number of items expressing sentiments classified as neither positive nor negative\n",
    "Positive: number of items expressing positive sentiments\n",
    "Very positive: number of items expressing strongly positive sentiments \n",
    "Negative: number of items expressing negative sentiments \n",
    "Very negative: number of items expressing strongly negative sentiments\n",
    "\"\"\"\n",
    "# brief analysis of sentiment and emotion distribution for the given movie\n",
    "analysis = collector.analyze_sentiment_and_emotion_distribution(data)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(\"Average sentiment scores:\")\n",
    "for k, v in analysis['sentiment_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for k, v in analysis['sentiment_counts'].items():\n",
    "    percentage = analysis['sentiment_percentages'][k]\n",
    "    print(f\"  {k}: {v} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available emotions in the pretrained model:\n",
      "0: joy\n",
      "1: optimism\n",
      "2: anger\n",
      "3: sadness\n",
      "Emotion analysis results:\n",
      "Average emotion scores:\n",
      "  Average joy emotion: 0.2830\n",
      "  Average optimism emotion: 0.3440\n",
      "  Average anger emotion: 0.1289\n",
      "  Average sadness emotion: 0.2380\n",
      "\n",
      "\n",
      "Top emotions:\n",
      "  Optimism: 5624.74 (34.61%)\n",
      "  Joy: 4627.25 (28.47%)\n",
      "  Sadness: 3891.73 (23.95%)\n",
      "  Anger: 2108.28 (12.97%)\n"
     ]
    }
   ],
   "source": [
    "# available emotions\n",
    "print(\"\\nAvailable emotions in the pretrained model:\")\n",
    "for i, label in enumerate(collector.emotion_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "# emotion analysis results\n",
    "print(\"Emotion analysis results:\")\n",
    "print(\"Average emotion scores:\")\n",
    "for k, v in analysis['emotion_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top emotions:\")\n",
    "emotions_data = analysis['emotions']\n",
    "for emotion, score in zip(\n",
    "    emotions_data['emotions'], \n",
    "    emotions_data['emotions_scores'].values()\n",
    "):\n",
    "    percentage = emotions_data['emotions_percentages'][emotion]\n",
    "    print(f\"  {emotion.capitalize()}: {score:.2f} ({percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
