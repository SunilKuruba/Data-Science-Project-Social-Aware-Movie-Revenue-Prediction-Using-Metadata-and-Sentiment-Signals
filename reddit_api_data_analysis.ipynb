{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Movie Sentiment Analysis - Data Collection / Sentiment + Emotion Analysis Script\n",
    "\n",
    "## Author: Leonardo Ferreira\n",
    "\n",
    "## 1. Objective\n",
    "The main goal is to collect and analyze sentiment and emotional responses to movies using reddit comments and posts.\n",
    "\n",
    "## 2. Sources\n",
    "- **Reddit API**: Data source\n",
    "- **Hugging Face Pretrained Models**: \n",
    "  - Sentiment Analysis: [Pretrained model for sentiment analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n",
    "  - Emotion Recognition: [Pretrained emotion recognition model](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base)\n",
    "\n",
    "## 3. Reddit API setup\n",
    "\n",
    "### 3.1 Create a reddit account\n",
    "\n",
    "- If you don't already have one, go to reddit's [registration page](https://www.reddit.com/register/)\n",
    "\n",
    "### 3.2 Create a reddit application\n",
    "\n",
    "- Go to your [app preferences page](https://www.reddit.com/prefs/apps) while logged in.\n",
    "- Scroll down to the bottom and click **\"create another app\"** (or **\"create app\"** if it's your first one).\n",
    "\n",
    "### 3.3 Fill in the application details\n",
    "\n",
    "- Select **\"script\"** as the application type.\n",
    "- Provide a name for your application (e.g., \"Movie Sentiment Analysis Project\").\n",
    "- Add a brief description.\n",
    "- For the **\"about url\"** and **\"redirect uri\"** fields, you can use `http://localhost:8080` as a placeholder.\n",
    "- Click **\"create app\"** to submit.\n",
    "\n",
    "### 3.4 Get your credentials\n",
    "\n",
    "- After creating the app, you'll see the **client ID** directly under the app name.\n",
    "- The **client secret** will be displayed as **\"secret\"**.\n",
    "- Make note of both, as you'll need them in your code.\n",
    "\n",
    "### 3.5 Example: Initializing the reddit API with PRAW\n",
    "\n",
    "```python\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "    user_agent=\"python:movie.sentiment_emotion.analyzer:v1.0 (by /u/your_username)\"\n",
    ")\n",
    "```\n",
    "\n",
    "## 4. Methodology\n",
    "\n",
    "1. **Data Retrieval**\n",
    "   - Utilize PRAW (python reddit API wrapper) to search reddit\n",
    "   - Search parameters include:\n",
    "     - Movie name as search query\n",
    "     - Relevance sorting\n",
    "     - Configurable time filter\n",
    "     - Limit on number of posts\n",
    "<br>\n",
    "2. **Text Preprocessing**\n",
    "   - Clean text by:\n",
    "     - Converting to lowercase\n",
    "     - Removing URLs\n",
    "     - Eliminating non-alphabetic characters\n",
    "     - Removing extra whitespace\n",
    "   - Process both post titles and body text\n",
    "   - Handle comments separately\n",
    "<br>\n",
    "3. **Sentiment Analysis**\n",
    "   - Use CardiffNLP's RoBERTa-based sentiment model\n",
    "   - Extract sentiment scores:\n",
    "     - Negative sentiment\n",
    "     - Neutral sentiment\n",
    "     - Positive sentiment\n",
    "     - Compound sentiment score\n",
    "<br>\n",
    "4. **Emotion Recognition**\n",
    "   - Apply DistilRoBERTa emotion recognition model\n",
    "   - Identify emotional categories:\n",
    "     - Anger\n",
    "     - Disgust\n",
    "     - Fear\n",
    "     - Joy\n",
    "     - Neutral\n",
    "     - Sadness\n",
    "     - Surprise\n",
    "<br>\n",
    "5. **Data Storage**\n",
    "   - Save processed data to CSV files\n",
    "   - Separate files for posts and comments\n",
    "   - Include:\n",
    "     - Original text\n",
    "     - Sentiment scores\n",
    "     - Emotion scores\n",
    "     - Metadata (author, timestamp, etc...)\n",
    "\n",
    "## 5. ML models\n",
    "\n",
    "### Sentiment analysis model\n",
    "- **Architecture**: RoBERTa\n",
    "- **Training Data**: Twitter\n",
    "- **Sentiment Categories**: 3 different sentiments (neutral, positive, negative)\n",
    "- **Output**: Prob. distribution across sentiments\n",
    "- **Compound Score**: Difference between positive and negative probs.\n",
    "\n",
    "### Emotion Recognition Model\n",
    "- **Architecture**: DistilRoBERTa\n",
    "- **Training Data**: English text\n",
    "- **Emotion Categories**: 7 different emotions\n",
    "- **Output**: Prob. distribution across emotions\n",
    "\n",
    "## 6. Considerations\n",
    "- **API Limitations**: \n",
    "  - Implement rate limiting\n",
    "- **Text Processing**:\n",
    "  - Truncate long texts to model's max length\n",
    "  - Clean and standardize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "# this is used to import private variables for reddit API\n",
    "# you can either modify the script using your own credentials or create a .env with them\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditMovieDataCollector:\n",
    "    def __init__(self, client_id, client_secret, user_agent):\n",
    "        \"\"\"\n",
    "        initialize the reddit API client\n",
    "        \n",
    "        parameters:\n",
    "        - client_id: your reddit API client ID\n",
    "        - client_secret: your reddit API client secret\n",
    "        - user_agent: unique identifier for your script\n",
    "        \"\"\"\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "\n",
    "\n",
    "        # initialize pretrained sentiment analysis model\n",
    "        # using fine-tuned model for sentiment analysis\n",
    "        # source: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "        self.sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model_name)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model_name)\n",
    "\n",
    "\n",
    "        # doing the same but for emotion analysis\n",
    "        # source: https://huggingface.co/j-hartmann/emotion-english-distilroberta-base\n",
    "        self.emotion_model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "        self.emotion_tokenizer = AutoTokenizer.from_pretrained(self.emotion_model_name)\n",
    "        self.emotion_model = AutoModelForSequenceClassification.from_pretrained(self.emotion_model_name)\n",
    "\n",
    "        # get emotion labels dynamically from the model configuration\n",
    "        self.emotion_labels = [\n",
    "            self.emotion_model.config.id2label[i] \n",
    "            for i in range(len(self.emotion_model.config.id2label))\n",
    "        ]\n",
    "\n",
    "        # create emotion column names\n",
    "        self.emotion_columns = [f\"{label.lower()}_emotion\" for label in self.emotion_labels]\n",
    "\n",
    "\n",
    "        # create folder for data if it doesn't exist\n",
    "        if not os.path.exists('movie_data_reddit'):\n",
    "            os.makedirs('movie_data_reddit')\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        clean and preprocess text data\n",
    "        \n",
    "        parameters:\n",
    "        - text: text to clean\n",
    "        \n",
    "        returns:\n",
    "        - cleaned text\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove urls\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # keep only letters and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_sentiment_scores(self, text):\n",
    "        \"\"\"\n",
    "        calculate sentiment scores for a given text using VADER\n",
    "        \n",
    "        parameters:\n",
    "        - text: text to analyze\n",
    "        \n",
    "        returns:\n",
    "        - dictionary with sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # return neutral sentiment if text is empty\n",
    "            return {\n",
    "                'compound': 0,\n",
    "                'pos': 0,\n",
    "                'neu': 1,\n",
    "                'neg': 0\n",
    "            }\n",
    "        \n",
    "        # truncate text if it's too long for the model\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        encoded_input = self.sentiment_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # get model output\n",
    "        with torch.no_grad():\n",
    "            output = self.sentiment_model(**encoded_input)\n",
    "        \n",
    "        # get probabilities\n",
    "        scores = softmax(output.logits[0].numpy())\n",
    "        \n",
    "        # map scores to sentiment categories (negative, neutral, positive)\n",
    "        sentiment_scores = {\n",
    "            'neg': float(scores[0]),\n",
    "            'neu': float(scores[1]),\n",
    "            'pos': float(scores[2]),\n",
    "            'compound': float(scores[2] - scores[0])\n",
    "        }\n",
    "        \n",
    "        return sentiment_scores\n",
    "    \n",
    "    def get_emotion_scores(self, text):\n",
    "        \"\"\"\n",
    "        calculate emotion scores\n",
    "\n",
    "        parameters:\n",
    "        - text: text to analyze\n",
    "        \n",
    "        returns:\n",
    "        - dictionary with emotion scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # return neutral emotion scores if text is empty\n",
    "            return {label.lower(): 0 for label in self.emotion_labels}\n",
    "        \n",
    "        # truncate text if it's too long for the model\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        encoded_input = self.emotion_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # get model output\n",
    "        with torch.no_grad():\n",
    "            output = self.emotion_model(**encoded_input)\n",
    "        \n",
    "        # get probabilities\n",
    "        scores = softmax(output.logits[0].numpy())\n",
    "        \n",
    "        # map scores to emotion categories dynamically\n",
    "        emotion_mapping = {\n",
    "            self.emotion_model.config.id2label[i].lower(): float(scores[i]) \n",
    "            for i in range(len(self.emotion_labels))\n",
    "        }\n",
    "        \n",
    "        return emotion_mapping\n",
    "    \n",
    "    def get_date_range(self, release_date, months_before):\n",
    "        \"\"\"\n",
    "        calculate a date range starting N months before a movie's release date\n",
    "        \n",
    "        parameters:\n",
    "        - release_date: release date datetime\n",
    "        - months_before: N of months before release date\n",
    "        \n",
    "        returns:\n",
    "        - start_date: datetime for the start date\n",
    "        - end_date: datetime for the end date (release date)\n",
    "        \"\"\"\n",
    "        # calculate start date (N months before release)\n",
    "        start_date = release_date - timedelta(days=30 * months_before)\n",
    "        \n",
    "        return start_date, release_date\n",
    "    \n",
    "    def collect_reddit_data(self, movie_name, release_date, months_before=3, limit=100):\n",
    "        \"\"\"\n",
    "        collect reddit data for a specific movie\n",
    "        \n",
    "        parameters:\n",
    "        - movie_name: name of the movie to search for\n",
    "        - release_date: movie's release date\n",
    "        - months_before: number of months before release date\n",
    "        - limit: max number of posts\n",
    "        \n",
    "        returns:\n",
    "        - df with collected data\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate date range\n",
    "        start_date, end_date = self.get_date_range(release_date, months_before)\n",
    "\n",
    "        # convert dates to unix timestamps\n",
    "        start_timestamp = int(start_date.timestamp())\n",
    "        end_timestamp = int(end_date.timestamp())\n",
    "        \n",
    "        posts_data = []\n",
    "        comments_data = []\n",
    "        \n",
    "        # search for posts related to the movie\n",
    "        search_query = movie_name\n",
    "\n",
    "        # to avoid infinite search we have to set a reasonable max limit to search for\n",
    "        max_posts_to_check = limit * 100\n",
    "        posts_processed = 0\n",
    "        post_count = 0\n",
    "        comment_count = 0\n",
    "        \n",
    "        for post in self.reddit.subreddit(\"all\").search(search_query, sort=\"relevance\", time_filter=\"all\", limit=None, syntax='lucene'):\n",
    "\n",
    "            # we need to break if checked too many posts without finding enough matches\n",
    "            if posts_processed > max_posts_to_check:\n",
    "                print(f\"maximum posts to check ({max_posts_to_check}) reached... stopping search\")\n",
    "                break\n",
    "\n",
    "            # we skip posts outside the date range we stablished\n",
    "            post_timestamp = post.created_utc\n",
    "            if post_timestamp < start_timestamp or post_timestamp > end_timestamp:\n",
    "                continue\n",
    "            \n",
    "            # if we reach the limit, then we can stop\n",
    "            if post_count >= limit:\n",
    "                break\n",
    "\n",
    "            post_count += 1\n",
    "            \n",
    "            # clean title and text\n",
    "            # for posts we use the TITLE and the POST TEXTUAL CONTENT for sentiment / emotion analysis\n",
    "            clean_title = self.clean_text(post.title)\n",
    "            clean_text = self.clean_text(post.selftext)\n",
    "            combined_text = f\"{clean_title} {clean_text}\"\n",
    "            \n",
    "            # get sentiment scores\n",
    "            sentiment_scores = self.get_sentiment_scores(combined_text)\n",
    "\n",
    "            # get emotion scores\n",
    "            emotion_scores = self.get_emotion_scores(combined_text)\n",
    "\n",
    "            # process post\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': str(post.author),\n",
    "                'score': post.score,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc),\n",
    "                'subreddit': post.subreddit.display_name,\n",
    "                'num_comments': post.num_comments,\n",
    "                'compound_sentiment': sentiment_scores['compound'],\n",
    "                'positive_sentiment': sentiment_scores['pos'],\n",
    "                'neutral_sentiment': sentiment_scores['neu'],\n",
    "                'negative_sentiment': sentiment_scores['neg'],\n",
    "                **{f\"{k}_emotion\": v for k, v in emotion_scores.items()},\n",
    "                'content_type': 'post'\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "            \n",
    "            # get comments while skipping loading more comments to avoid API rate limits\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for comment in post.comments.list():\n",
    "                # filter comments by date as well\n",
    "                comment_timestamp = comment.created_utc\n",
    "                if comment_timestamp < start_timestamp or comment_timestamp > end_timestamp:\n",
    "                    continue\n",
    "                comment_count += 1\n",
    "                \n",
    "                # clean comment text\n",
    "                # for commentws we use the COMMENT TEXTUAL BODY for sentiment / emotion analysis\n",
    "                clean_comment = self.clean_text(comment.body)\n",
    "                \n",
    "                # get sentiment scores\n",
    "                comment_sentiment = self.get_sentiment_scores(clean_comment)\n",
    "\n",
    "                # get emotion scores\n",
    "                comment_emotion = self.get_emotion_scores(clean_comment)\n",
    "\n",
    "                # process comment\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'post_id': post.id,\n",
    "                    'text': comment.body,\n",
    "                    'author': str(comment.author),\n",
    "                    'score': comment.score,\n",
    "                    'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "                    'subreddit': post.subreddit.display_name,\n",
    "                    'compound_sentiment': comment_sentiment['compound'],\n",
    "                    'positive_sentiment': comment_sentiment['pos'],\n",
    "                    'neutral_sentiment': comment_sentiment['neu'],\n",
    "                    'negative_sentiment': comment_sentiment['neg'],\n",
    "                    **{f\"{k}_emotion\": v for k, v in comment_emotion.items()},\n",
    "                    'content_type': 'comment'\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "            \n",
    "            # sleep to avoid rate limits\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # create dfs\n",
    "        posts_df = pd.DataFrame(posts_data)\n",
    "        comments_df = pd.DataFrame(comments_data)\n",
    "        \n",
    "        # save to csv\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        movie_name_cleaned = movie_name.replace(\" \", \"_\").lower()\n",
    "        \n",
    "        posts_filename = f\"movie_data_reddit/{movie_name_cleaned}_posts.csv\"\n",
    "        comments_filename = f\"movie_data_reddit/{movie_name_cleaned}_comments.csv\"\n",
    "        \n",
    "        posts_df.to_csv(posts_filename, index=False, quoting=1, escapechar='\\\\')\n",
    "        comments_df.to_csv(comments_filename, index=False, quoting=1, escapechar='\\\\')\n",
    "        \n",
    "        print(f\"Data collection is complete: {post_count} posts and {comment_count} comments\")\n",
    "        print(f\"Data saved to {posts_filename} and {comments_filename}\")\n",
    "        \n",
    "        # combine data for analysis\n",
    "        all_data = pd.concat([posts_df, comments_df])\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def analyze_sentiment_and_emotion_distribution(self, data):\n",
    "        \"\"\"\n",
    "        analyze the sentiment and emotion distribution of collected data\n",
    "        \n",
    "        parameters:\n",
    "        - data: df with collected data\n",
    "        \n",
    "        returns:\n",
    "        - df with sentiment and emotion distribution analysis\n",
    "        \"\"\"\n",
    "        # calculate averages for sentiment scores\n",
    "        sentiment_avg = {\n",
    "            'Average compound score': data['compound_sentiment'].mean(),\n",
    "            'Average positive score': data['positive_sentiment'].mean(),\n",
    "            'Average neutral score': data['neutral_sentiment'].mean(),\n",
    "            'Average negative score': data['negative_sentiment'].mean()\n",
    "        }\n",
    "        \n",
    "        # categorize sentiments\n",
    "        data['dominant_sentiment'] = data[['negative_sentiment', 'neutral_sentiment', 'positive_sentiment']].idxmax(axis=1)\n",
    "\n",
    "        # map the column names to more readable sentiment names\n",
    "        sentiment_name_mapping = {\n",
    "            'negative_sentiment': 'Negative',\n",
    "            'neutral_sentiment': 'Neutral',\n",
    "            'positive_sentiment': 'Positive'\n",
    "        }\n",
    "\n",
    "        # apply the mapping\n",
    "        data['sentiment_category'] = data['dominant_sentiment'].map(sentiment_name_mapping)\n",
    "\n",
    "        # count by category\n",
    "        sentiment_counts = data['sentiment_category'].value_counts().to_dict()\n",
    "        \n",
    "        # calculate percentages\n",
    "        total = sum(sentiment_counts.values())\n",
    "        sentiment_percentages = {k: (v / total) * 100 for k, v in sentiment_counts.items()}\n",
    "        \n",
    "        # emotion analysis\n",
    "        emotion_columns = [col for col in data.columns if col.endswith('_emotion')]\n",
    "\n",
    "        # calculate average emotion scores\n",
    "        emotion_avg = {f'Average {col.split(\"_\")[0]} emotion': data[col].mean() for col in emotion_columns}\n",
    "        \n",
    "        # calculate total emotion scores across all content\n",
    "        total_emotion_scores = {}\n",
    "        for col in emotion_columns:\n",
    "            emotion_name = col.split('_')[0]\n",
    "            total_emotion_scores[emotion_name] = data[col].sum()\n",
    "        \n",
    "        # sort emotions\n",
    "        sorted_emotions = sorted(total_emotion_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        emotions_dict = {\n",
    "            'emotions': [emotion for emotion, score in sorted_emotions],\n",
    "            'emotions_scores': {emotion: score for emotion, score in sorted_emotions}\n",
    "        }\n",
    "\n",
    "        # calculate percentages for top 5 emotions\n",
    "        total_emotion_score = sum(total_emotion_scores.values())\n",
    "        emotions_percentages = {\n",
    "            emotion: (score / total_emotion_score) * 100 \n",
    "            for emotion, score in emotions_dict['emotions_scores'].items()\n",
    "        }\n",
    "        emotions_dict['emotions_percentages'] = emotions_percentages\n",
    "\n",
    "        # combine results\n",
    "        analysis_result = {\n",
    "            'sentiment_avg': sentiment_avg,\n",
    "            'sentiment_counts': sentiment_counts,\n",
    "            'sentiment_percentages': sentiment_percentages,\n",
    "            'emotion_avg': emotion_avg,\n",
    "            'emotions': emotions_dict\n",
    "        }\n",
    "        \n",
    "        return analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load credentials from .env file\n",
    "load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "# initialize collector with reddit credentials\n",
    "collector = RedditMovieDataCollector(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection is complete: 50 posts and 16997 comments\n",
      "Data saved to movie_data_reddit/oppenheimer_posts.csv and movie_data_reddit/oppenheimer_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# collecting data for Oppenheimer\n",
    "# collect data for a given movie\n",
    "# change the movie name to get sentiments about different movies\n",
    "movie_name = \"Oppenheimer\"\n",
    "oppenheimer_release = datetime(2023, 7, 21)\n",
    "# limit = 50 means: 50 posts. Be aware that a post can have multiple comments (API rate limit)\n",
    "# moreover, since the API does not provide a way to search between date ranges, we need to do it by brute force\n",
    "# the algorithm will try up to limit * 100 searches to reach the number of posts\n",
    "# however, if it does not reach it will break to avoid an infinite search\n",
    "limit = 50\n",
    "months_before = 3\n",
    "oppenheimer_data = collector.collect_reddit_data(movie_name, oppenheimer_release, months_before, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis results:\n",
      "Average sentiment scores:\n",
      "  Average compound score: -0.1287\n",
      "  Average positive score: 0.2230\n",
      "  Average neutral score: 0.4253\n",
      "  Average negative score: 0.3517\n",
      "\n",
      "\n",
      "Sentiment distribution:\n",
      "  Neutral: 7699 (45.16%)\n",
      "  Negative: 6134 (35.98%)\n",
      "  Positive: 3214 (18.85%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metrics meaning\n",
    "\n",
    "Sentiment scores:\n",
    "Average compound score: overall sentiment score, ranging from -1 (very negative) to +1 (very positive)\n",
    "Average positive score: represents the proportion of text that expresses positive sentiment\n",
    "Average neutral score: % of the content that is presenting emotionally neutral language\n",
    "Average negative score: % of the content that is presenting emotionally negative language\n",
    "\n",
    "Sentiment distribution:\n",
    "Neutral: number of items expressing sentiments classified as neither positive nor negative\n",
    "Positive: number of items expressing positive sentiments\n",
    "Very positive: number of items expressing strongly positive sentiments \n",
    "Negative: number of items expressing negative sentiments \n",
    "Very negative: number of items expressing strongly negative sentiments\n",
    "\"\"\"\n",
    "# brief analysis of sentiment and emotion distribution for the given movie\n",
    "oppenheimer_analysis = collector.analyze_sentiment_and_emotion_distribution(oppenheimer_data)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(\"Average sentiment scores:\")\n",
    "for k, v in oppenheimer_analysis['sentiment_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for k, v in oppenheimer_analysis['sentiment_counts'].items():\n",
    "    percentage = oppenheimer_analysis['sentiment_percentages'][k]\n",
    "    print(f\"  {k}: {v} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available emotions in the pretrained model:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: joy\n",
      "4: neutral\n",
      "5: sadness\n",
      "6: surprise\n",
      "Emotion analysis results:\n",
      "Average emotion scores:\n",
      "  Average anger emotion: 0.1094\n",
      "  Average disgust emotion: 0.0402\n",
      "  Average fear emotion: 0.0537\n",
      "  Average joy emotion: 0.1560\n",
      "  Average neutral emotion: 0.3085\n",
      "  Average sadness emotion: 0.1206\n",
      "  Average surprise emotion: 0.2064\n",
      "\n",
      "\n",
      "Top emotions:\n",
      "  Neutral: 5258.96 (31.01%)\n",
      "  Surprise: 3518.86 (20.75%)\n",
      "  Joy: 2660.05 (15.69%)\n",
      "  Sadness: 2056.03 (12.12%)\n",
      "  Anger: 1865.00 (11.00%)\n",
      "  Fear: 915.62 (5.40%)\n",
      "  Disgust: 684.49 (4.04%)\n"
     ]
    }
   ],
   "source": [
    "# available emotions\n",
    "print(\"\\nAvailable emotions in the pretrained model:\")\n",
    "for i, label in enumerate(collector.emotion_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "# emotion analysis results\n",
    "print(\"Emotion analysis results:\")\n",
    "print(\"Average emotion scores:\")\n",
    "for k, v in oppenheimer_analysis['emotion_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top emotions:\")\n",
    "emotions_data = oppenheimer_analysis['emotions']\n",
    "for emotion, score in zip(\n",
    "    emotions_data['emotions'], \n",
    "    emotions_data['emotions_scores'].values()\n",
    "):\n",
    "    percentage = emotions_data['emotions_percentages'][emotion]\n",
    "    print(f\"  {emotion.capitalize()}: {score:.2f} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection is complete: 50 posts and 14877 comments\n",
      "Data saved to movie_data_reddit/deadpool_&_wolverine_posts.csv and movie_data_reddit/deadpool_&_wolverine_comments.csv\n",
      "Sentiment analysis results:\n",
      "Average sentiment scores:\n",
      "  Average compound score: -0.0576\n",
      "  Average positive score: 0.2499\n",
      "  Average neutral score: 0.4425\n",
      "  Average negative score: 0.3075\n",
      "\n",
      "\n",
      "Sentiment distribution:\n",
      "  Neutral: 7109 (47.63%)\n",
      "  Negative: 4543 (30.43%)\n",
      "  Positive: 3275 (21.94%)\n",
      "\n",
      "Available emotions in the pretrained model:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: joy\n",
      "4: neutral\n",
      "5: sadness\n",
      "6: surprise\n",
      "Emotion analysis results:\n",
      "Average emotion scores:\n",
      "  Average anger emotion: 0.0834\n",
      "  Average disgust emotion: 0.0358\n",
      "  Average fear emotion: 0.0349\n",
      "  Average joy emotion: 0.1870\n",
      "  Average neutral emotion: 0.3100\n",
      "  Average sadness emotion: 0.1255\n",
      "  Average surprise emotion: 0.2190\n",
      "\n",
      "\n",
      "Top emotions:\n",
      "  Neutral: 4627.44 (31.14%)\n",
      "  Surprise: 3268.94 (22.00%)\n",
      "  Joy: 2791.35 (18.78%)\n",
      "  Sadness: 1873.88 (12.61%)\n",
      "  Anger: 1244.39 (8.37%)\n",
      "  Disgust: 533.99 (3.59%)\n",
      "  Fear: 521.01 (3.51%)\n"
     ]
    }
   ],
   "source": [
    "# collecting data for Deadpool & Wolverine\n",
    "movie_name = \"Deadpool & Wolverine\"\n",
    "deadpool_wolverine_release = datetime(2024, 7, 26)\n",
    "deadwolv_data = collector.collect_reddit_data(movie_name, deadpool_wolverine_release, months_before, limit)\n",
    "\n",
    "deadwolv_analysis = collector.analyze_sentiment_and_emotion_distribution(deadwolv_data)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(\"Average sentiment scores:\")\n",
    "for k, v in deadwolv_analysis['sentiment_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for k, v in deadwolv_analysis['sentiment_counts'].items():\n",
    "    percentage = deadwolv_analysis['sentiment_percentages'][k]\n",
    "    print(f\"  {k}: {v} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nAvailable emotions in the pretrained model:\")\n",
    "for i, label in enumerate(collector.emotion_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "print(\"Emotion analysis results:\")\n",
    "print(\"Average emotion scores:\")\n",
    "for k, v in deadwolv_analysis['emotion_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top emotions:\")\n",
    "emotions_data = deadwolv_analysis['emotions']\n",
    "for emotion, score in zip(\n",
    "    emotions_data['emotions'], \n",
    "    emotions_data['emotions_scores'].values()\n",
    "):\n",
    "    percentage = emotions_data['emotions_percentages'][emotion]\n",
    "    print(f\"  {emotion.capitalize()}: {score:.2f} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection is complete: 31 posts and 7252 comments\n",
      "Data saved to movie_data_reddit/flash_(2023)_posts.csv and movie_data_reddit/flash_(2023)_comments.csv\n",
      "Sentiment analysis results:\n",
      "Average sentiment scores:\n",
      "  Average compound score: -0.2093\n",
      "  Average positive score: 0.1895\n",
      "  Average neutral score: 0.4118\n",
      "  Average negative score: 0.3987\n",
      "\n",
      "\n",
      "Sentiment distribution:\n",
      "  Negative: 3084 (42.35%)\n",
      "  Neutral: 3028 (41.58%)\n",
      "  Positive: 1171 (16.08%)\n",
      "\n",
      "Available emotions in the pretrained model:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: joy\n",
      "4: neutral\n",
      "5: sadness\n",
      "6: surprise\n",
      "Emotion analysis results:\n",
      "Average emotion scores:\n",
      "  Average anger emotion: 0.1200\n",
      "  Average disgust emotion: 0.0493\n",
      "  Average fear emotion: 0.0577\n",
      "  Average joy emotion: 0.1368\n",
      "  Average neutral emotion: 0.3150\n",
      "  Average sadness emotion: 0.1305\n",
      "  Average surprise emotion: 0.1861\n",
      "\n",
      "\n",
      "Top emotions:\n",
      "  Neutral: 2293.99 (31.65%)\n",
      "  Surprise: 1355.52 (18.70%)\n",
      "  Joy: 996.21 (13.74%)\n",
      "  Sadness: 950.11 (13.11%)\n",
      "  Anger: 873.82 (12.05%)\n",
      "  Fear: 420.32 (5.80%)\n",
      "  Disgust: 359.03 (4.95%)\n"
     ]
    }
   ],
   "source": [
    "# collecting data for Flash (2023)\n",
    "movie_name = \"Flash (2023)\"\n",
    "flash_release = datetime(2023, 6, 16)\n",
    "flash_data = collector.collect_reddit_data(movie_name, flash_release, months_before, limit)\n",
    "\n",
    "flash_analysis = collector.analyze_sentiment_and_emotion_distribution(flash_data)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(\"Average sentiment scores:\")\n",
    "for k, v in flash_analysis['sentiment_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for k, v in flash_analysis['sentiment_counts'].items():\n",
    "    percentage = flash_analysis['sentiment_percentages'][k]\n",
    "    print(f\"  {k}: {v} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nAvailable emotions in the pretrained model:\")\n",
    "for i, label in enumerate(collector.emotion_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "print(\"Emotion analysis results:\")\n",
    "print(\"Average emotion scores:\")\n",
    "for k, v in flash_analysis['emotion_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top emotions:\")\n",
    "emotions_data = flash_analysis['emotions']\n",
    "for emotion, score in zip(\n",
    "    emotions_data['emotions'], \n",
    "    emotions_data['emotions_scores'].values()\n",
    "):\n",
    "    percentage = emotions_data['emotions_percentages'][emotion]\n",
    "    print(f\"  {emotion.capitalize()}: {score:.2f} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection is complete: 50 posts and 9070 comments\n",
      "Data saved to movie_data_reddit/joker:_folie_a_deux_posts.csv and movie_data_reddit/joker:_folie_a_deux_comments.csv\n",
      "Sentiment analysis results:\n",
      "Average sentiment scores:\n",
      "  Average compound score: -0.2138\n",
      "  Average positive score: 0.1956\n",
      "  Average neutral score: 0.3951\n",
      "  Average negative score: 0.4094\n",
      "\n",
      "\n",
      "Sentiment distribution:\n",
      "  Negative: 3992 (43.77%)\n",
      "  Neutral: 3658 (40.11%)\n",
      "  Positive: 1470 (16.12%)\n",
      "\n",
      "Available emotions in the pretrained model:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: joy\n",
      "4: neutral\n",
      "5: sadness\n",
      "6: surprise\n",
      "Emotion analysis results:\n",
      "Average emotion scores:\n",
      "  Average anger emotion: 0.1026\n",
      "  Average disgust emotion: 0.0458\n",
      "  Average fear emotion: 0.0475\n",
      "  Average joy emotion: 0.1802\n",
      "  Average neutral emotion: 0.2690\n",
      "  Average sadness emotion: 0.1426\n",
      "  Average surprise emotion: 0.2027\n",
      "\n",
      "\n",
      "Top emotions:\n",
      "  Neutral: 2452.86 (27.16%)\n",
      "  Surprise: 1848.66 (20.47%)\n",
      "  Joy: 1643.45 (18.20%)\n",
      "  Sadness: 1300.94 (14.40%)\n",
      "  Anger: 935.57 (10.36%)\n",
      "  Fear: 432.85 (4.79%)\n",
      "  Disgust: 417.67 (4.62%)\n"
     ]
    }
   ],
   "source": [
    "# collecting data for Joker: Folie à Deux\n",
    "movie_name = \"Joker: Folie a Deux\"\n",
    "joker_release = datetime(2024, 10, 4)\n",
    "joker_data = collector.collect_reddit_data(movie_name, joker_release, months_before, limit)\n",
    "\n",
    "joker_analysis = collector.analyze_sentiment_and_emotion_distribution(joker_data)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(\"Average sentiment scores:\")\n",
    "for k, v in joker_analysis['sentiment_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for k, v in joker_analysis['sentiment_counts'].items():\n",
    "    percentage = joker_analysis['sentiment_percentages'][k]\n",
    "    print(f\"  {k}: {v} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nAvailable emotions in the pretrained model:\")\n",
    "for i, label in enumerate(collector.emotion_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "print(\"Emotion analysis results:\")\n",
    "print(\"Average emotion scores:\")\n",
    "for k, v in joker_analysis['emotion_avg'].items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top emotions:\")\n",
    "emotions_data = joker_analysis['emotions']\n",
    "for emotion, score in zip(\n",
    "    emotions_data['emotions'], \n",
    "    emotions_data['emotions_scores'].values()\n",
    "):\n",
    "    percentage = emotions_data['emotions_percentages'][emotion]\n",
    "    print(f\"  {emotion.capitalize()}: {score:.2f} ({percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
