{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Movie Sentiment Analysis - Data Collection / Sentiment + Emotion Analysis Script\n",
    "\n",
    "## Author: Leonardo Ferreira\n",
    "\n",
    "## 1. Objective\n",
    "The main goal is to collect and analyze sentiment and emotional responses to movies using reddit comments and posts.\n",
    "\n",
    "## 2. Sources\n",
    "- **Reddit API**: Data source\n",
    "- **Hugging Face Pretrained Models**: \n",
    "  - Sentiment Analysis: [Pretrained model for sentiment analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n",
    "  - Emotion Recognition: [Pretrained emotion recognition model](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base)\n",
    "\n",
    "## 3. Reddit API setup\n",
    "\n",
    "### 3.1 Create a reddit account\n",
    "\n",
    "- If you don't already have one, go to reddit's [registration page](https://www.reddit.com/register/)\n",
    "\n",
    "### 3.2 Create a reddit application\n",
    "\n",
    "- Go to your [app preferences page](https://www.reddit.com/prefs/apps) while logged in.\n",
    "- Scroll down to the bottom and click **\"create another app\"** (or **\"create app\"** if it's your first one).\n",
    "\n",
    "### 3.3 Fill in the application details\n",
    "\n",
    "- Select **\"script\"** as the application type.\n",
    "- Provide a name for your application (e.g., \"Movie Sentiment Analysis Project\").\n",
    "- Add a brief description.\n",
    "- For the **\"about url\"** and **\"redirect uri\"** fields, you can use `http://localhost:8080` as a placeholder.\n",
    "- Click **\"create app\"** to submit.\n",
    "\n",
    "### 3.4 Get your credentials\n",
    "\n",
    "- After creating the app, you'll see the **client ID** directly under the app name.\n",
    "- The **client secret** will be displayed as **\"secret\"**.\n",
    "- Make note of both, as you'll need them in your code.\n",
    "\n",
    "### 3.5 Example: Initializing the reddit API with PRAW\n",
    "\n",
    "```python\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "    user_agent=\"python:movie.sentiment_emotion.analyzer:v1.0 (by /u/your_username)\"\n",
    ")\n",
    "```\n",
    "\n",
    "## 4. Methodology\n",
    "\n",
    "1. **Data Retrieval**\n",
    "   - Utilize PRAW (python reddit API wrapper) to search reddit\n",
    "   - Search parameters include:\n",
    "     - Movie name as search query\n",
    "     - Relevance sorting\n",
    "     - Configurable time filter\n",
    "     - Limit on number of posts\n",
    "<br>\n",
    "2. **Text Preprocessing**\n",
    "   - Clean text by:\n",
    "     - Converting to lowercase\n",
    "     - Removing URLs\n",
    "     - Eliminating non-alphabetic characters\n",
    "     - Removing extra whitespace\n",
    "   - Process both post titles and body text\n",
    "   - Handle comments separately\n",
    "<br>\n",
    "3. **Sentiment Analysis**\n",
    "   - Use CardiffNLP's RoBERTa-based sentiment model\n",
    "   - Extract sentiment scores:\n",
    "     - Negative sentiment\n",
    "     - Neutral sentiment\n",
    "     - Positive sentiment\n",
    "     - Compound sentiment score\n",
    "<br>\n",
    "4. **Emotion Recognition**\n",
    "   - Apply DistilRoBERTa emotion recognition model\n",
    "   - Identify emotional categories:\n",
    "     - Anger\n",
    "     - Disgust\n",
    "     - Fear\n",
    "     - Joy\n",
    "     - Neutral\n",
    "     - Sadness\n",
    "     - Surprise\n",
    "<br>\n",
    "5. **Data Storage**\n",
    "   - Save processed data to CSV files\n",
    "   - Separate files for posts and comments\n",
    "   - Include:\n",
    "     - Original text\n",
    "     - Sentiment scores\n",
    "     - Emotion scores\n",
    "     - Metadata (author, timestamp, etc...)\n",
    "    \n",
    "## 5. ML models\n",
    "\n",
    "### Sentiment analysis model\n",
    "- **Architecture**: RoBERTa\n",
    "- **Training Data**: Twitter\n",
    "- **Sentiment Categories**: 3 different sentiments (neutral, positive, negative)\n",
    "- **Output**: Prob. distribution across sentiments\n",
    "- **Compound Score**: Difference between positive and negative probs.\n",
    "\n",
    "### Emotion Recognition Model\n",
    "- **Architecture**: DistilRoBERTa\n",
    "- **Training Data**: English text\n",
    "- **Emotion Categories**: 7 different emotions\n",
    "- **Output**: Prob. distribution across emotions\n",
    "\n",
    "## 6. Considerations\n",
    "- **API Limitations**: \n",
    "  - Implement rate limiting\n",
    "  - Can't use date ranges to limit the search\n",
    "  \n",
    "- **Text Processing**:\n",
    "  - Truncate long texts to model's max length\n",
    "  - Clean and standardize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "# this is used to import private variables for reddit API\n",
    "# you can either modify the script using your own credentials or create a .env with them\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditMovieDataCollector:\n",
    "    def __init__(self, client_id, client_secret, user_agent):\n",
    "        \"\"\"\n",
    "        initialize the reddit API client\n",
    "        \n",
    "        parameters:\n",
    "        - client_id: your reddit API client ID\n",
    "        - client_secret: your reddit API client secret\n",
    "        - user_agent: unique identifier for your script\n",
    "        \"\"\"\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "\n",
    "\n",
    "        # initialize pretrained sentiment analysis model\n",
    "        # using fine-tuned model for sentiment analysis\n",
    "        # source: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "        self.sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model_name)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model_name)\n",
    "\n",
    "\n",
    "        # doing the same but for emotion analysis\n",
    "        # source: https://huggingface.co/j-hartmann/emotion-english-distilroberta-base\n",
    "        self.emotion_model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "        self.emotion_tokenizer = AutoTokenizer.from_pretrained(self.emotion_model_name)\n",
    "        self.emotion_model = AutoModelForSequenceClassification.from_pretrained(self.emotion_model_name)\n",
    "\n",
    "        # get emotion labels dynamically from the model configuration\n",
    "        self.emotion_labels = [\n",
    "            self.emotion_model.config.id2label[i] \n",
    "            for i in range(len(self.emotion_model.config.id2label))\n",
    "        ]\n",
    "\n",
    "        # create emotion column names\n",
    "        self.emotion_columns = [f\"{label.lower()}_emotion\" for label in self.emotion_labels]\n",
    "\n",
    "\n",
    "        # create folder for data if it doesn't exist\n",
    "        if not os.path.exists('movie_data_reddit'):\n",
    "            os.makedirs('movie_data_reddit')\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        clean and preprocess text data\n",
    "        \n",
    "        parameters:\n",
    "        - text: text to clean\n",
    "        \n",
    "        returns:\n",
    "        - cleaned text\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove urls\n",
    "        text = re.sub(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', '', text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_sentiment_scores(self, text):\n",
    "        \"\"\"\n",
    "        calculate sentiment scores for a given text using VADER\n",
    "        \n",
    "        parameters:\n",
    "        - text: text to analyze\n",
    "        \n",
    "        returns:\n",
    "        - dictionary with sentiment scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # return neutral sentiment if text is empty\n",
    "            return {\n",
    "                'compound': 0,\n",
    "                'pos': 0,\n",
    "                'neu': 1,\n",
    "                'neg': 0\n",
    "            }\n",
    "        \n",
    "        # truncate text if it's too long for the model\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        encoded_input = self.sentiment_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # get model output\n",
    "        with torch.no_grad():\n",
    "            output = self.sentiment_model(**encoded_input)\n",
    "        \n",
    "        # get probabilities\n",
    "        scores = softmax(output.logits[0].numpy())\n",
    "        \n",
    "        # map scores to sentiment categories (negative, neutral, positive)\n",
    "        sentiment_scores = {\n",
    "            'neg': float(scores[0]),\n",
    "            'neu': float(scores[1]),\n",
    "            'pos': float(scores[2]),\n",
    "            'compound': float(scores[2] - scores[0])\n",
    "        }\n",
    "        \n",
    "        return sentiment_scores\n",
    "    \n",
    "    def get_emotion_scores(self, text):\n",
    "        \"\"\"\n",
    "        calculate emotion scores\n",
    "\n",
    "        parameters:\n",
    "        - text: text to analyze\n",
    "        \n",
    "        returns:\n",
    "        - dictionary with emotion scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            # return neutral emotion scores if text is empty\n",
    "            return {label.lower(): 0 for label in self.emotion_labels}\n",
    "        \n",
    "        # truncate text if it's too long for the model\n",
    "        max_length = 512\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        encoded_input = self.emotion_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # get model output\n",
    "        with torch.no_grad():\n",
    "            output = self.emotion_model(**encoded_input)\n",
    "        \n",
    "        # get probabilities\n",
    "        scores = softmax(output.logits[0].numpy())\n",
    "        \n",
    "        # map scores to emotion categories dynamically\n",
    "        emotion_mapping = {\n",
    "            self.emotion_model.config.id2label[i].lower(): float(scores[i]) \n",
    "            for i in range(len(self.emotion_labels))\n",
    "        }\n",
    "        \n",
    "        return emotion_mapping\n",
    "    \n",
    "    def get_date_range(self, release_date, months_before):\n",
    "        \"\"\"\n",
    "        calculate a date range starting N months before a movie's release date\n",
    "        \n",
    "        parameters:\n",
    "        - release_date: release date datetime\n",
    "        - months_before: N of months before release date\n",
    "        \n",
    "        returns:\n",
    "        - start_date: datetime for the start date\n",
    "        - end_date: datetime for the end date (release date)\n",
    "        \"\"\"\n",
    "        # calculate start date (N months before release)\n",
    "        start_date = release_date - timedelta(days=30 * months_before)\n",
    "        \n",
    "        return start_date, release_date\n",
    "    \n",
    "    def collect_reddit_data(self, movie_name, release_date, months_before, limit):\n",
    "        \"\"\"\n",
    "        collect reddit data for a specific movie\n",
    "        \n",
    "        parameters:\n",
    "        - movie_name: name of the movie to search for\n",
    "        - release_date: movie's release date\n",
    "        - months_before: number of months before release date\n",
    "        - limit: max number of posts\n",
    "        \n",
    "        returns:\n",
    "        - df with collected data\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate date range\n",
    "        start_date, end_date = self.get_date_range(release_date, months_before)\n",
    "\n",
    "        # convert dates to unix timestamps\n",
    "        start_timestamp = int(start_date.timestamp())\n",
    "        end_timestamp = int(end_date.timestamp())\n",
    "        \n",
    "        posts_data = []\n",
    "        comments_data = []\n",
    "        \n",
    "        # search for posts related to the movie\n",
    "        search_query = movie_name\n",
    "\n",
    "        # to avoid infinite search we have to set a reasonable max limit to search for\n",
    "        max_posts_to_check = limit * 100\n",
    "        posts_processed = 0\n",
    "        post_count = 0\n",
    "        comment_count = 0\n",
    "        \n",
    "        for post in self.reddit.subreddit(\"all\").search(search_query, sort=\"relevance\", time_filter=\"all\", limit=None, syntax='lucene'):\n",
    "\n",
    "            # we need to break if checked too many posts without finding enough matches\n",
    "            if posts_processed > max_posts_to_check:\n",
    "                print(f\"maximum posts to check ({max_posts_to_check}) reached... stopping search\")\n",
    "                break\n",
    "\n",
    "            # we skip posts outside the date range we stablished\n",
    "            post_timestamp = post.created_utc\n",
    "            if post_timestamp < start_timestamp or post_timestamp > end_timestamp:\n",
    "                continue\n",
    "            \n",
    "            # if we reach the limit, then we can stop\n",
    "            if post_count >= limit:\n",
    "                break\n",
    "\n",
    "            post_count += 1\n",
    "            \n",
    "            # clean title and text\n",
    "            # for posts we use the TITLE and the POST TEXTUAL CONTENT for sentiment / emotion analysis\n",
    "            clean_title = self.clean_text(post.title)\n",
    "            clean_text = self.clean_text(post.selftext)\n",
    "            combined_text = f\"{clean_title} {clean_text}\"\n",
    "            \n",
    "            # get sentiment scores\n",
    "            sentiment_scores = self.get_sentiment_scores(combined_text)\n",
    "\n",
    "            # get emotion scores\n",
    "            emotion_scores = self.get_emotion_scores(combined_text)\n",
    "\n",
    "            # process post\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'author': str(post.author),\n",
    "                'score': post.score,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc),\n",
    "                'subreddit': post.subreddit.display_name,\n",
    "                'num_comments': post.num_comments,\n",
    "                'compound_sentiment': sentiment_scores['compound'],\n",
    "                'positive_sentiment': sentiment_scores['pos'],\n",
    "                'neutral_sentiment': sentiment_scores['neu'],\n",
    "                'negative_sentiment': sentiment_scores['neg'],\n",
    "                **{f\"{k}_emotion\": v for k, v in emotion_scores.items()},\n",
    "                'content_type': 'post'\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "            \n",
    "            # get comments while skipping loading more comments to avoid API rate limits\n",
    "            post.comments.replace_more(limit=0)\n",
    "\n",
    "            # get top comments based on its score\n",
    "            top_comments = list(post.comments)\n",
    "            top_comments.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "            post_comment_count = 0\n",
    "\n",
    "            for comment in top_comments:\n",
    "                # filter comments by date as well\n",
    "                comment_timestamp = comment.created_utc\n",
    "                if comment_timestamp < start_timestamp or comment_timestamp > end_timestamp:\n",
    "                    continue\n",
    "\n",
    "                # filter by minimum length (80 characters)\n",
    "                if len(comment.body) < 80:\n",
    "                    continue\n",
    "                \n",
    "                # clean comment text\n",
    "                # for commentws we use the COMMENT TEXTUAL BODY for sentiment / emotion analysis\n",
    "                clean_comment = self.clean_text(comment.body)\n",
    "                \n",
    "                # get sentiment scores\n",
    "                comment_sentiment = self.get_sentiment_scores(clean_comment)\n",
    "\n",
    "                # get emotion scores\n",
    "                comment_emotion = self.get_emotion_scores(clean_comment)\n",
    "\n",
    "                # process comment\n",
    "                comment_data = {\n",
    "                    'id': comment.id,\n",
    "                    'post_id': post.id,\n",
    "                    'text': comment.body,\n",
    "                    'author': str(comment.author),\n",
    "                    'score': comment.score,\n",
    "                    'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "                    'subreddit': post.subreddit.display_name,\n",
    "                    'compound_sentiment': comment_sentiment['compound'],\n",
    "                    'positive_sentiment': comment_sentiment['pos'],\n",
    "                    'neutral_sentiment': comment_sentiment['neu'],\n",
    "                    'negative_sentiment': comment_sentiment['neg'],\n",
    "                    **{f\"{k}_emotion\": v for k, v in comment_emotion.items()},\n",
    "                    'content_type': 'comment'\n",
    "                }\n",
    "                comments_data.append(comment_data)\n",
    "\n",
    "                comment_count += 1\n",
    "                post_comment_count += 1\n",
    "\n",
    "                # lets limit it to 20 comments per post\n",
    "                if post_comment_count >= 20:\n",
    "                    break\n",
    "\n",
    "            # sleep to avoid rate limits\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # create dfs\n",
    "        posts_df = pd.DataFrame(posts_data)\n",
    "        comments_df = pd.DataFrame(comments_data)\n",
    "        \n",
    "        # save to csv\n",
    "        movie_name_cleaned = movie_name.replace(\" \", \"_\").lower()\n",
    "        \n",
    "        posts_filename = f\"movie_data_reddit/{movie_name_cleaned}_posts.csv\"\n",
    "        comments_filename = f\"movie_data_reddit/{movie_name_cleaned}_comments.csv\"\n",
    "        \n",
    "        posts_df.to_csv(posts_filename, index=False, quoting=1, escapechar='\\\\')\n",
    "        comments_df.to_csv(comments_filename, index=False, quoting=1, escapechar='\\\\')\n",
    "        \n",
    "        print(f\"Data collection is complete: {post_count} posts and {comment_count} comments\")\n",
    "        print(f\"Data saved to {posts_filename} and {comments_filename}\")\n",
    "        \n",
    "        # combine data for analysis\n",
    "        all_data = pd.concat([posts_df, comments_df])\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def process_movies_with_incremental_saving(self, movies_df, output_file='tmdb_movies_with_reddit_sentiment_emotion_scores.csv', limit=20, months_before=1):\n",
    "        \"\"\"\n",
    "        process movies and add sentiment/emotion scores with incremental saving\n",
    "        \n",
    "        Parameters:\n",
    "        - collector: RedditMovieDataCollector instance\n",
    "        - movies_df: df containing movie data\n",
    "        - output_file: File to save results to\n",
    "        - limit: Max num of posts to collect per movie\n",
    "        - months_before: Num of months before release to collect data\n",
    "        \n",
    "        returns:\n",
    "        - updated movies df with sentiment and emotion scores\n",
    "        \"\"\"\n",
    "        # check if output file exists and load it if it does\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"found existing file: {output_file}\")\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "\n",
    "            # get the list of movies already processed\n",
    "            processed_movies = existing_df['movie_ID'].tolist() if 'movie_ID' in existing_df.columns else []\n",
    "            if processed_movies:\n",
    "                print(f\"found {len(processed_movies)} already processed movies, we will start from where we stopped\")\n",
    "\n",
    "                # filter out movies that have already been processed\n",
    "                movies_to_process = movies_df[~movies_df['movie_ID'].isin(processed_movies)]\n",
    "                \n",
    "                # combine the existing data with the new data we're going to process\n",
    "                result_df = existing_df\n",
    "            else:\n",
    "                movies_to_process = movies_df\n",
    "                result_df = pd.DataFrame()\n",
    "        else:\n",
    "            # otherwise, it means that we are starting the data download for the first time\n",
    "            print(f\"creating new file: {output_file}\")\n",
    "            movies_to_process = movies_df\n",
    "            \n",
    "            # start new df\n",
    "            # plus the sentiment and emotion columns we'll add\n",
    "            result_df = pd.DataFrame()\n",
    "        \n",
    "        # skip processing if all movies have been processed\n",
    "        if len(movies_to_process) == 0:\n",
    "            print(\"all movies processed\")\n",
    "            return\n",
    "        \n",
    "        print(f\"processing {len(movies_to_process)} movies...\")\n",
    "        \n",
    "        # Define sentiment and emotion categories\n",
    "        sentiment_categories = ['negative', 'neutral', 'positive']\n",
    "        emotion_categories = [label.lower() for label in self.emotion_labels]\n",
    "        \n",
    "        # process each movie at time\n",
    "        for index, row in movies_to_process.iterrows():\n",
    "            print(f\"processing movie {index+1}/{len(movies_to_process)}: {row['title']}\")\n",
    "            \n",
    "            # copy of the current row that we'll update with sentiment data\n",
    "            movie_row = row.copy()\n",
    "            \n",
    "            # sentiment and emotion scores for this movie\n",
    "            for category in sentiment_categories:\n",
    "                movie_row[f'{category}_sentiment'] = 0.0\n",
    "            \n",
    "            for category in emotion_categories:\n",
    "                movie_row[f'{category}_emotion'] = 0.0\n",
    "            \n",
    "            # parse date\n",
    "            release_date = datetime.strptime(row['Release Date'], '%Y-%m-%d')\n",
    "            \n",
    "            # get reddit data for current movie\n",
    "            reddit_data = self.collect_reddit_data(\n",
    "                row['title'], \n",
    "                release_date, \n",
    "                months_before=months_before, \n",
    "                limit=limit\n",
    "            )\n",
    "        \n",
    "            # calculate sentiment and emotion scores\n",
    "            for category in sentiment_categories:\n",
    "                movie_row[f'{category}_sentiment'] = reddit_data[f'{category}_sentiment'].mean()\n",
    "            for category in emotion_categories:\n",
    "                movie_row[f'{category}_emotion'] = reddit_data[f'{category}_emotion'].mean()\n",
    "            \n",
    "            # convert the row to a df\n",
    "            movie_df = pd.DataFrame([movie_row])\n",
    "            \n",
    "            # append it to existing df \n",
    "            if result_df.empty:\n",
    "                result_df = movie_df\n",
    "            else:\n",
    "                result_df = pd.concat([result_df, movie_df], ignore_index=True)\n",
    "            \n",
    "            # save for every row\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load credentials from .env file\n",
    "load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "# read data\n",
    "movies_df = pd.read_csv('tmdb_1000_sample.csv')\n",
    "#movies_df = movies_df.head(5)\n",
    "\n",
    "# initialize collector with reddit credentials\n",
    "collector = RedditMovieDataCollector(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT)\n",
    "\n",
    "# collect data and add sentiment and emotions to it \n",
    "output_file = 'reddit_movie_data/tmdb_movies_with_reddit_sentiment_emotion_scores.csv'\n",
    "movies_with_sentiment_and_emotion = collector.process_movies_with_incremental_saving(\n",
    "    movies_df, # data\n",
    "    output_file, # output file name\n",
    "    20,  # limit\n",
    "    1  # months before\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
